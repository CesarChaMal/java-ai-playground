# Ollama provider
langchain4j.base-url=${OLLAMA_BASE_URL:http://localhost:11434}

# Chat model
langchain4j.ollama.chat-model.model-name=${OLLAMA_CHAT_MODEL:mistral}
langchain4j.ollama.chat-model.temperature=0.0

# Embeddings (optional)
langchain4j.ollama.embedding-model.model-name=${OLLAMA_EMBEDDING_MODEL:nomic-embed-text}

# NEW ? which model name to use for the OpenAiTokenizer (for token counting)
langchain4j.ollama.tokenizer.model-name=gpt-4o-mini

# Debugging for Ollama
logging.level.dev.langchain4j=DEBUG