# Ollama provider
langchain4j.base-url=http://localhost:11434

# Chat model
langchain4j.ollama.chat-model.model-name=mistral
langchain4j.ollama.chat-model.temperature=0.0

# Embeddings (optional)
# langchain4j.ollama.embedding-model.model-name=mistral
langchain4j.ollama.embedding-model.model-name=nomic-embed-text

# NEW ? which model name to use for the OpenAiTokenizer (for token counting)
langchain4j.ollama.tokenizer.model-name=gpt-4o-mini

# Debugging for Ollama
logging.level.dev.langchain4j=DEBUG